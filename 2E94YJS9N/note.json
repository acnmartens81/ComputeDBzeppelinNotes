{
  "paragraphs": [
    {
      "text": "%spark\n/*\n * Copyright (c) 2018 SnappyData, Inc. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you\n * may not use this file except in compliance with the License. You\n * may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n * implied. See the License for the specific language governing\n * permissions and limitations under the License. See accompanying\n * LICENSE file.\n */\n\n// package org.apache.spark.examples.snappydata\n\nimport java.io.File\nimport java.lang.{Integer \u003d\u003e JInt}\nimport java.net.InetSocketAddress\nimport java.util.concurrent.TimeUnit\nimport java.util.{Properties, Map \u003d\u003e JMap}\n\nimport kafka.admin.AdminUtils\nimport kafka.api.Request\nimport kafka.server.{KafkaConfig, KafkaServer}\nimport kafka.utils.ZkUtils\nimport org.apache.kafka.clients.consumer.KafkaConsumer\nimport org.apache.kafka.clients.producer.{KafkaProducer, Producer, ProducerRecord, RecordMetadata}\nimport org.apache.kafka.common.TopicPartition\nimport org.apache.kafka.common.serialization.{StringDeserializer, StringSerializer}\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.SparkConf\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.jdbc.{ConnectionConfBuilder, ConnectionUtil}\nimport org.apache.spark.sql.streaming.{SchemaDStream, StreamToRowsConverter}\nimport org.apache.spark.sql.{Row, SparkSession}\nimport org.apache.spark.streaming.{Seconds, SnappyStreamingContext}\nimport org.apache.spark.util.Utils\nimport org.apache.zookeeper.server.{NIOServerCnxnFactory, ZooKeeperServer}\nimport org.json4s.NoTypeHints\nimport org.json4s.jackson.Serialization\n\nimport scala.collection.JavaConverters._\nimport scala.collection.mutable.HashMap\nimport scala.language.postfixOps\nimport scala.util.Random\nimport scala.util.control.NonFatal\n\n/**\n  * An example showing usage of streaming with SnappyData\n  *\n  * \u003cp\u003e\u003c/p\u003e\n  * To run the example in local mode go to your SnappyData product distribution\n  * directory and type following command on the command prompt\n  * \u003cpre\u003e\n  * bin/run-example snappydata.StreamingExample\n  * \u003c/pre\u003e\n  *\n  * This example starts embedded Kafka and publishes messages(advertising bids)\n  * on it to be processed in SnappyData as streaming data. In SnappyData, streams\n  * are managed declaratively by creating a stream table(adImpressionStream). A\n  * continuous query is executed on the stream and its result is processed and\n  * publisher_bid_counts table is modified based on the streaming data\n  *\n  * We also update a row table to maintain the no of distinct bids so far(example\n  * of storing and updating a state of streaming data)\n  *\n  * For more details on streaming with SnappyData refer to:\n  * http://snappydatainc.github.io/snappydata/programming_guide\n  * /stream_processing_using_sql/#stream-processing-using-sql\n  *\n  */\nobject StreamingExample {\n\n  def main(args: Array[String]) {\n    // reducing the log level to minimize the messages on console\n    Logger.getLogger(\"org\").setLevel(Level.ERROR)\n    Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n\n    val dataDirAbsolutePath \u003d createAndGetDataDir\n\n    println(\"Initializing a SnappyStreamingContext\")\n    val spark: SparkSession \u003d SparkSession\n      .builder\n      .appName(getClass.getSimpleName)\n      .master(\"local[*]\")\n      // sys-disk-dir attribute specifies the directory where persistent data is saved\n      .config(\"snappydata.store.sys-disk-dir\", dataDirAbsolutePath)\n      .config(\"snappydata.store.log-file\", dataDirAbsolutePath + \"/SnappyDataExample.log\")\n      .getOrCreate\n\n    val snsc \u003d new SnappyStreamingContext(spark.sparkContext, Seconds(1))\n\n    println()\n    println(\"Initializing embedded Kafka\")\n    val utils \u003d new EmbeddedKafkaUtils()\n    utils.setup()\n    val topic \u003d \"kafka_topic\"\n    utils.createTopic(topic)\n\n    val add \u003d utils.brokerAddress\n    val groupId \u003d s\"test-consumer-\" + Random.nextInt(10000)\n\n    println()\n    println(\"Creating a stream table to read data from Kafka\")\n    snsc.sql(\"drop table if exists adImpressionStream\")\n    snsc.sql(\"drop table if exists publisher_bid_counts\")\n\n    // Streams can be managed as tables declaratively using SQL\n    // statements.\n    // rowConverter attribute specifies a class that converts a\n    // stream message into a Row object\n    // for more details on stream tables refer to\n    // http://snappydatainc.github.io/snappydata/streamingWithSQL/\n    snsc.sql(\n      \"create stream table adImpressionStream (\" +\n        \" time_stamp timestamp,\" +\n        \" publisher string,\" +\n        \" advertiser string,\" +\n        \" website string,\" +\n        \" geo string,\" +\n        \" bid double,\" +\n        \" cookie string) \" + \" using kafka_stream options(\" +\n        \" rowConverter \u0027org.apache.spark.examples.snappydata.RowsConverter\u0027,\" +\n        s\" kafkaParams \u0027bootstrap.servers-\u003e$add;\" +\n        \"key.deserializer-\u003eorg.apache.kafka.common.serialization.StringDeserializer;\" +\n        \"value.deserializer-\u003eorg.apache.kafka.common.serialization.StringDeserializer;\" +\n        s\"group.id-\u003e$groupId;auto.offset.reset-\u003eearliest\u0027,\" +\n        \" startingOffsets \u0027{\\\"\" + topic + \"\\\":{\\\"0\\\":0}}\u0027, \" +\n        s\" subscribe \u0027$topic\u0027)\"\n    )\n\n    // create a row table that will maintain no of bids per publisher\n    snsc.sql(\"create table publisher_bid_counts(publisher string, bidCount int) using row\")\n    snsc.sql(\"insert into publisher_bid_counts values(\u0027publisher1\u0027, 0)\")\n    snsc.sql(\"insert into publisher_bid_counts values(\u0027publisher2\u0027, 0)\")\n    snsc.sql(\"insert into publisher_bid_counts values(\u0027publisher3\u0027, 0)\")\n    snsc.sql(\"insert into publisher_bid_counts values(\u0027publisher4\u0027, 0)\")\n\n    println()\n    // Execute this query once every second. Output is a SchemaDStream.\n    println(\"Registering a continuous query to to be executed every second on the stream table\")\n    val resultStream: SchemaDStream \u003d snsc.registerCQ(\"select publisher, count(bid) as bidCount from \" +\n      \"adImpressionStream window (duration 1 seconds, slide 1 seconds) group by publisher\")\n\n    // this conf is used to get a connection a JDBC connection\n    val conf \u003d new ConnectionConfBuilder(snsc.snappySession).build\n    println()\n\n    // process the stream data returned by continuous query and update publisher_bid_counts table\n    resultStream.foreachDataFrame(df \u003d\u003e {\n      if (df.count() \u003e 0L) {\n        println(\"Data received in streaming window\")\n        df.show()\n\n        println(\"Updating table publisher_bid_counts\")\n        val conn \u003d ConnectionUtil.getConnection(conf)\n        val result \u003d df.collect()\n        val stmt \u003d conn.prepareStatement(\"update publisher_bid_counts set \" +\n          s\"bidCount \u003d bidCount + ? where publisher \u003d ?\")\n\n        result.foreach(row \u003d\u003e {\n          val publisher \u003d row.getString(0)\n          val bidCount \u003d row.getLong(1)\n          stmt.setLong(1, bidCount)\n          stmt.setString(2, publisher)\n          stmt.addBatch()\n        }\n        )\n        stmt.executeBatch()\n        conn.close()\n      } else {\n        println(\"No data received in streaming window\")\n      }\n    })\n\n    snsc.start\n\n    println(\"Publishing messages on Kafka\")\n    publishKafkaMessages(utils, topic)\n\n    Thread.sleep(3000)\n\n    println(\"***Total no of bids per publisher are***\")\n    snsc.snappySession.sql(\"select publisher, bidCount from publisher_bid_counts\").show()\n\n    println(\"Exiting\")\n    snsc.stop(false)\n    utils.teardown()\n    System.exit(0)\n  }\n\n  def createAndGetDataDir: String \u003d {\n    // creating a directory to save all persistent data\n    val dataDir \u003d \"./\" + \"snappydata_examples_data\"\n    new File(dataDir).mkdir()\n    val dataDirAbsolutePath \u003d new File(dataDir).getAbsolutePath\n    dataDirAbsolutePath\n  }\n\n  def publishKafkaMessages(utils: EmbeddedKafkaUtils, topic: String): Unit \u003d {\n    for (_ \u003c- 0 until 10) {\n      val currentTime \u003d System.currentTimeMillis()\n\n      // bids with comma separated fields\n      //timestamp, publisher,advertiser,web,geo,bid,cookie\n      val bid1 \u003d currentTime + \",publisher1,advt1,pb1.web,US,\" + scala.util.Random.nextDouble() + \",23543\"\n      val bid2 \u003d currentTime + \",publisher2,advt1,pb1.web,US,\" + scala.util.Random.nextDouble() + \",45445\"\n      val bid3 \u003d currentTime + \",publisher3,advt2,pb1.web,US,\" + scala.util.Random.nextDouble() + \",13434\"\n      val bid4 \u003d currentTime + \",publisher4,advt2,pb1.web,US,\" + scala.util.Random.nextDouble() + \",34324\"\n      val bid5 \u003d currentTime + \",publisher2,advt1,pb1.web,US,\" + scala.util.Random.nextDouble() + \",23233\"\n      val bid6 \u003d currentTime + \",publisher4,advt2,pb1.web,US,\" + scala.util.Random.nextDouble() + \",43545\"\n\n      // publish the bids as a Kafka message\n      utils.sendMessages(topic, Array(bid1, bid2, bid3, bid4, bid5, bid6))\n      println(\"Published message containing 6 rows\")\n      Thread.sleep(1000)\n    }\n\n    println(\"Done publishing all messages\")\n\n  }\n}\n\n\n/**\n  * Converts an input stream message into an org.apache.spark.sql.Row\n  * instance\n  */\nclass RowsConverter extends StreamToRowsConverter with Serializable {\n\n  override def toRows(message: Any): Seq[Row] \u003d {\n    val log \u003d message.asInstanceOf[String]\n    val fields \u003d log.split(\",\")\n    Seq(Row.fromSeq(Seq(new java.sql.Timestamp(fields(0).toLong),\n      fields(1),\n      fields(2),\n      fields(3),\n      fields(4),\n      fields(5).toDouble,\n      fields(6)\n    )))\n  }\n}\n\nclass EmbeddedKafkaUtils extends Logging {\n\n  // Zookeeper related configurations\n  private val zkHost \u003d \"localhost\"\n  private var zkPort: Int \u003d 0\n  private val zkConnectionTimeout \u003d 60000\n  private val zkSessionTimeout \u003d 6000\n\n  private var zookeeper: EmbeddedZookeeper \u003d _\n\n  private var zkUtils: kafka.utils.ZkUtils \u003d _\n\n  // Kafka broker related configurations\n  private val brokerHost \u003d \"localhost\"\n  private var brokerPort \u003d 0\n  private var brokerConf: KafkaConfig \u003d _\n\n  // Kafka broker server\n  private var server: KafkaServer \u003d _\n\n  // Kafka producer\n  private var producer: Producer[String, String] \u003d _\n\n  // Flag to test whether the system is correctly started\n  private var zkReady \u003d false\n  private var brokerReady \u003d false\n\n  def zkAddress: String \u003d {\n    assert(zkReady, \"Zookeeper not setup yet or already torn down, cannot get zookeeper address\")\n    s\"$zkHost:$zkPort\"\n  }\n\n  def brokerAddress: String \u003d {\n    assert(brokerReady, \"Kafka not setup yet or already torn down, cannot get broker address\")\n    s\"$brokerHost:$brokerPort\"\n  }\n\n  def zookeeperClient: ZkUtils \u003d {\n    assert(zkReady, \"Zookeeper not setup yet or already torn down, cannot get zookeeper client\")\n    Option(zkUtils).getOrElse(\n      throw new IllegalStateException(\"Zookeeper client is not yet initialized\"))\n  }\n\n  // Set up the Embedded Zookeeper server and get the proper Zookeeper port\n  private def setupEmbeddedZookeeper(): Unit \u003d {\n    // Zookeeper server startup\n    zookeeper \u003d new EmbeddedZookeeper(s\"$zkHost:$zkPort\")\n    // Get the actual zookeeper binding port\n    zkPort \u003d zookeeper.actualPort\n    zkUtils \u003d ZkUtils(s\"$zkHost:$zkPort\", zkSessionTimeout, zkConnectionTimeout, false)\n    zkReady \u003d true\n  }\n\n  // Set up the Embedded Kafka server\n  private def setupEmbeddedKafkaServer(): Unit \u003d {\n    assert(zkReady, \"Zookeeper should be set up beforehand\")\n\n    // Kafka broker startup\n    Utils.startServiceOnPort(brokerPort, port \u003d\u003e {\n      brokerPort \u003d port\n      brokerConf \u003d new KafkaConfig(brokerConfiguration, doLog \u003d false)\n      server \u003d new KafkaServer(brokerConf)\n      server.startup()\n      brokerPort \u003d server.boundPort()\n      (server, brokerPort)\n    }, new SparkConf(), \"KafkaBroker\")\n\n    brokerReady \u003d true\n  }\n\n  /** setup the whole embedded servers, including Zookeeper and Kafka brokers */\n  def setup(): Unit \u003d {\n    setupEmbeddedZookeeper()\n    setupEmbeddedKafkaServer()\n  }\n\n  /** Teardown the whole servers, including Kafka broker and Zookeeper */\n  def teardown(): Unit \u003d {\n    brokerReady \u003d false\n    zkReady \u003d false\n\n    if (producer !\u003d null) {\n      producer.close()\n      producer \u003d null\n    }\n\n    if (server !\u003d null) {\n      server.shutdown()\n      server \u003d null\n    }\n\n    brokerConf.logDirs.foreach { f \u003d\u003e Utils.deleteRecursively(new File(f)) }\n\n    if (zkUtils !\u003d null) {\n      zkUtils.close()\n      zkUtils \u003d null\n    }\n\n    if (zookeeper !\u003d null) {\n      zookeeper.shutdown()\n      zookeeper \u003d null\n    }\n  }\n\n  /** Create a Kafka topic and wait until it is propagated to the whole cluster */\n  def createTopic(topic: String, partitions: Int, overwrite: Boolean \u003d false): Unit \u003d {\n    var created \u003d false\n    while (!created) {\n      try {\n        AdminUtils.createTopic(zkUtils, topic, partitions, 1)\n        created \u003d true\n      } catch {\n        case e: kafka.common.TopicExistsException if overwrite \u003d\u003e // deleteTopic(topic)\n      }\n    }\n    // wait until metadata is propagated\n    (0 until partitions).foreach { p \u003d\u003e\n      waitUntilMetadataIsPropagated(topic, p)\n    }\n  }\n\n  /** Create a Kafka topic and wait until it is propagated to the whole cluster */\n  def createTopic(topic: String): Unit \u003d {\n    createTopic(topic, 1)\n  }\n\n\n  /** Send the array of messages to the Kafka broker */\n  def sendMessages(topic: String, messages: Array[String]): Seq[(String, RecordMetadata)] \u003d {\n    sendMessages(topic, messages, None)\n  }\n\n  /** Send the array of messages to the Kafka broker using specified partition */\n  def sendMessages(\n                    topic: String,\n                    messages: Array[String],\n                    partition: Option[Int]): Seq[(String, RecordMetadata)] \u003d {\n    producer \u003d new KafkaProducer[String, String](producerConfiguration)\n    val offsets \u003d try {\n      messages.map { m \u003d\u003e\n        val record \u003d partition match {\n          case Some(p) \u003d\u003e new ProducerRecord[String, String](topic, p, null, m)\n          case None \u003d\u003e new ProducerRecord[String, String](topic, m)\n        }\n        val metadata \u003d\n          producer.send(record).get(10, TimeUnit.SECONDS)\n        (m, metadata)\n      }\n    } finally {\n      if (producer !\u003d null) {\n        producer.close()\n        producer \u003d null\n      }\n    }\n    offsets\n  }\n\n  protected def brokerConfiguration: Properties \u003d {\n    val props \u003d new Properties()\n    props.put(\"broker.id\", \"0\")\n    props.put(\"host.name\", \"localhost\")\n    props.put(\"advertised.host.name\", \"localhost\")\n    props.put(\"port\", brokerPort.toString)\n    props.put(\"log.dir\", Utils.createTempDir().getAbsolutePath)\n    props.put(\"zookeeper.connect\", zkAddress)\n    props.put(\"log.flush.interval.messages\", \"1\")\n    props.put(\"replica.socket.timeout.ms\", \"1500\")\n    props.put(\"delete.topic.enable\", \"true\")\n    props\n  }\n\n  private def producerConfiguration: Properties \u003d {\n    val props \u003d new Properties()\n    props.put(\"bootstrap.servers\", brokerAddress)\n    props.put(\"value.serializer\", classOf[StringSerializer].getName)\n    props.put(\"key.serializer\", classOf[StringSerializer].getName)\n    // wait for all in-sync replicas to ack sends\n    props.put(\"acks\", \"all\")\n    props\n  }\n\n  private def waitUntilMetadataIsPropagated(topic: String, partition: Int): Unit \u003d {\n    def isPropagated \u003d server.apis.metadataCache.getPartitionInfo(topic, partition) match {\n      case Some(partitionState) \u003d\u003e\n        val leaderAndInSyncReplicas \u003d partitionState.leaderIsrAndControllerEpoch.leaderAndIsr\n\n        zkUtils.getLeaderForPartition(topic, partition).isDefined \u0026\u0026\n          Request.isValidBrokerId(leaderAndInSyncReplicas.leader) \u0026\u0026\n          leaderAndInSyncReplicas.isr.size \u003e\u003d 1\n\n      case _ \u003d\u003e\n        false\n    }\n  }\n\n  private class EmbeddedZookeeper(val zkConnect: String) {\n    val snapshotDir \u003d Utils.createTempDir()\n    val logDir \u003d Utils.createTempDir()\n\n    val zookeeper \u003d new ZooKeeperServer(snapshotDir, logDir, 500)\n    val (ip, port) \u003d {\n      val splits \u003d zkConnect.split(\":\")\n      (splits(0), splits(1).toInt)\n    }\n    val factory \u003d new NIOServerCnxnFactory()\n    factory.configure(new InetSocketAddress(ip, port), 16)\n    factory.startup(zookeeper)\n\n    val actualPort \u003d factory.getLocalPort\n\n    def shutdown() {\n      factory.shutdown()\n      Utils.deleteRecursively(snapshotDir)\n      Utils.deleteRecursively(logDir)\n    }\n  }\n\n}",
      "user": "anonymous",
      "dateUpdated": "Apr 11, 2019 5:48:26 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/text"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1555004883183_-1699456929",
      "id": "20190411-174803_796694146",
      "dateCreated": "Apr 11, 2019 5:48:03 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Streaming Example",
  "id": "2E94YJS9N",
  "angularObjects": {
    "2EA979ZH6:existing_process": [],
    "2E8BS2UKF:shared_process": [],
    "2EA2P371D:shared_process": []
  },
  "config": {},
  "info": {}
}